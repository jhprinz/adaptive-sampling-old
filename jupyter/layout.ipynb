{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queueing System\n",
    "\n",
    "Allow MD simulations to be scheduled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to implement this and we should keep specific aspects in mind:\n",
    "\n",
    "1. We do not know, if (a) python will run on the cluster and (b) the cluster can connect to the internet\n",
    "2. You want to run on your local machine for testing and with little changes on a cluster\n",
    "3. You might want to use workers that pull a job from a list or put jobs in a queue. \n",
    "4. We want to be able to use different simulation engines: Gromacs, AceMD, OpenMM, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    class Strategy(StorableMixin):\n",
    "        pass    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be great if a user could define a strategy using simple building blocks. I presume this will be too rigid and too complicated. If we provide a ways that makes it wasy to write a little python program, that does what we want, it might be better. I could also imagine that you need to subclass a `Strategy` class and implement certain functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to express a strategy by a user\n",
    "\n",
    "1. provide high-level function with lots of options\n",
    "\n",
    "    ```py\n",
    "    strat = OneOverCAdaptiveStrategy(trajs='all', prior=1)\n",
    "    ```\n",
    "\n",
    "2. use building blocks to define a acyclic directed graph (ADC)\n",
    "\n",
    "    ```py\n",
    "    strat = {\n",
    "        'traj_set': (storage.trajectories.load, slice(None, None)),\n",
    "        'correlation_matrix': (pyemma.get_correlation, 'traj_set', 'featurizer'),\n",
    "        'tica_proj': (pyemma.magic_tica_fnc, 'correlation_matrix', ),\n",
    "        'clustering': (pyemma.magic_clustering_fnc, 'tica_proj', 'traj_set'),\n",
    "        'count_matrix': (pyemma.count_fnc, 'clustering', 'traj_set'),\n",
    "        'msm': (pyemma.ml_rev_msm, 'count_matrix'),\n",
    "    }\n",
    "    ```\n",
    "\n",
    "3. use building blocks to create a quasi-Domain Specific Language (DSL) that expresses 2. but is better readable\n",
    "\n",
    "    ```py\n",
    "    Sequence([\n",
    "        Add(\n",
    "            MSM(\n",
    "                trajs=Storage.trajectories)),\n",
    "        Add(\n",
    "            Clustering())\n",
    "    ])\n",
    "    ```\n",
    "    \n",
    "4. write python code that is translated into 2. (if possible) and gets executed using the scheduler\n",
    "\n",
    "    ```py\n",
    "    \n",
    "    ```\n",
    "    \n",
    "5. write python code that uses the schedular and has helper functions to access the existing results\n",
    "\n",
    "    ```py\n",
    "    \n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    storage = MongoDBStorage('alanine_adaptive')\n",
    "    queue = WorkerCeleryQS(storage=storage)\n",
    "    strategy = RespawnAtOneOverC()\n",
    "    sampler = AdaptiveSampling(storage, queue, strategy)\n",
    "    sampler.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main routine that combines all aspects.\n",
    "\n",
    "1. The queueing system\n",
    "2. A place to store objects\n",
    "3. The logic to build models\n",
    "4. The application of the adaptive strategy\n",
    "\n",
    "Once you have created all these parts you can create the sampler and run it. The QS and the ST should exist independently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyEmma (build MSMs)\n",
    "\n",
    "Construct an MSM from given data in the DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    storage = MongoDBStorage('alanine_adaptive')\n",
    "\n",
    "    storage.trajectories.save(last_trajectory)\n",
    "\n",
    "    print len(storage.trajectories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will remember the number of items per store at a certain point in time. If you want to store the result of a query you can simply remember the query function and the storage state to reconstruct the actual result. Very useful to keep track of new objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StorageState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special Iterators\n",
    "\n",
    "A way to encode all trajectories of a specific type in the storage. Usually you would write\n",
    "\n",
    "```py\n",
    "stateA_trajs = [traj for traj in storage.trajectories if traj[0] in stateA]\n",
    "```\n",
    "\n",
    "we might want to simplify this into\n",
    "\n",
    "```\n",
    "all_traj_iterator = storage.trajectories\n",
    "stateA_traj_iterator = storage.trajectories.filter(lambda traj: traj[0] in stateA) \n",
    "```\n",
    "\n",
    "These queries should also be storable as well as the result as implicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Externally stored content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Functionalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ExternalTrajectory\n",
    "\n",
    "Instead of saving a list of snapshots we could just save a whole trajectory as one object and reference it by filename. It would work exactly as a normal trajectory but the snapshots will be loaded specifically and iterating over a trajectory is also different. \n",
    "\n",
    "Not sure how to get a uuid for the snapshots then but I guess you can do that by skipping $n$ indices when saving the trajectory. Then, if the traj has id `17` and is of length 5. The frames have UUID `18 - 22`. These snapshots can be accessed from a special externalsnapshot store that can handle the requests.\n",
    "\n",
    "#### ExternalSnapshot\n",
    "\n",
    "These are referencing a frame number and an `MDFile`\n",
    "\n",
    "#### ExternalFile\n",
    "\n",
    "Referencing an external filenme with a UUID. A filename should also be unique, but this way it is unified. You can also reference other URI like websites, etc...\n",
    "\n",
    "#### MDFile\n",
    "\n",
    "An external file readable by MDTraj\n",
    "\n",
    "There are some caveats to make sure this is efficient but the main point is bookkeeping and that should be possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix\n",
    "\n",
    "In general a matrix with a special purpose and references on how they were created, e.g. `CorrelationMatrix`, `TransitionMatrix`, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional StorableObjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep it simple for a user we should create a simple API to instantiate a `QueueingSystem` which only a few options. This QS can be adapted to all sorts of other clusters by subclassing. The main point is that the QS can \n",
    "\n",
    "1. accept new tasks to run\n",
    "2. add the result of the tasks to a storage\n",
    "3. report on the status of task execution\n",
    "4. controle the execution, i.e., abort gracefully, remove tasks, etc...\n",
    "\n",
    "all the rest is handled by the specific QS instance. The QS should be responsibe for the distribution and execution, and if desired also the setup of necessary workers or initial preparations, etc.\n",
    "\n",
    "##### Example 1\n",
    "\n",
    "Assume you want to use workers on a cluster, then you need to run the server that workers can connect to and place the worker in the cue.\n",
    "\n",
    "##### Example 2\n",
    "\n",
    "Placing jobs in a cue might not require any additional preparations beside monitoring the cue and see if you can place new jobs in the cue or when a job is finished doing some cleaning up and registering the result in the DB.\n",
    "\n",
    "#### Storing\n",
    "\n",
    "After a task is finished its results need to be registered in the DB. Either you store a pointer to the files \n",
    "\n",
    "```py\n",
    "storage.trajectories.save(ExternalMDTrajectory('file0001.xtc'))\n",
    "```\n",
    "\n",
    "or save it as a full trajectory object in the DB copying all frames with it\n",
    "\n",
    "```py\n",
    "storage.trajectories.save(Trajectory(iterator_of_frames))\n",
    "```\n",
    "\n",
    "Still, the QS is responsible to do that with whatever the worker returned. If it returned the file location then you might reference it (i.e. Gromacs), if it returns a stream of a pickled trajectory store it (i.e. OpenMM).\n",
    "\n",
    "#### Persistance\n",
    "\n",
    "The QS should (not necessary) run independent of the main task and be able to be stopped or continued. The idea is that the actual jobs to be done and the execution are independent. The Sampler will add tasks to the queue and if you get time on a cluster you can reduce the list. As long as you have not run these you can still clear the queue or add more jobs. For the MSM Adaptive Scheme we usually do not have dependent tasks and it does never matter in which order we run the tasks or if we skip some. If we pick good candidated we will converge faster, but we will not converge to a wrong solution (within the bounds of the projection error or course).\n",
    "\n",
    "#### Additional features\n",
    "\n",
    "- add automatic retries, if a simulation should crash or is aborted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # return a pyemma source object\n",
    "    storage.trajectories.as_source  \n",
    "\n",
    "    # actually .trajectories is an iterator over all trajectories \n",
    "\n",
    "    # [Do all the pyemma magic]\n",
    "\n",
    "    storage.models.save(model)  # should contain all references to how its created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    orchestrator = ClusterQueueingSystem(\n",
    "        storage=ClusterStorage(),\n",
    "        config_file,\n",
    "        [options])\n",
    "\n",
    "    simulation_task = SimulateNFramesMDTask(\n",
    "        engine = my_gromacs_engine,\n",
    "        start_frame = my_last_frame,\n",
    "        n_frames = 100\n",
    "    )\n",
    "\n",
    "    # do this 100 times\n",
    "    orchestrator.append([simulation_task] * 100)\n",
    "\n",
    "    print orchestrator.currently_running_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResultDB\n",
    "\n",
    "Store all data / results from the queueing system, model builder, etc.\n",
    "\n",
    "The database is some kind of repository that does the bookkeeping. This does not necessarily involve storing all trajetory files, but all existing files should be mentioned in the database. The same should be true for all models, tasks, clusterings, etc. All objects that might be of later value and we want to access easily.\n",
    "\n",
    "Since there are several ways to do that we should just provide an API and do several implementations for different purposes. \n",
    "\n",
    "I propose either to \n",
    "\n",
    "1. adopt the netCDF+ approach, which is basically a NoSQL DB in a single file and extend it to point to external files (a PR already exists, but needs to be updates),\n",
    "2. to use a MongoDB instead which is the more general approach, but require a MongoDB server to be running.\n",
    "3. use a special directory and store one file per object which a specific naming scheme\n",
    "\n",
    "But all will share the same API which I would adopt from what we use with OPS. \n",
    "\n",
    "The first two are more elegant and provide the additional benefit of easier search and access as well as reusing existing objects. In this case it is also important to use strictly immutable objects, ob objects where only non-essential attributes are mutable.\n",
    "\n",
    "If objects are immutable we can safely use pointers to existing objects without the danger that these might change in the future. This will simplify storage immensely.\n",
    "\n",
    "The most important point is that the storage is persisting and will not disappear once a job is finished or the main simulation crashed. It should also be suited to restart a simulation either after crashing or if more simulation is needed. Lastly it can also serve as the starting point for further simulations and analysis.\n",
    "\n",
    "A DB is best suited to remain consistant, whereas the file based approaches can suffer if the simulation crashes at the wrong moment.\n",
    "\n",
    "For the purely filebased approach we need to write functions that parse the directory tree and return the appropriate objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy\n",
    "\n",
    "The Brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still the most experimental and not clear part. We might have to try different ideas to get something that feels right."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
